{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0caaf755-ab02-482f-a38f-00e6c32106db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img\n",
    "    src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\"\n",
    "    alt=\"Databricks Learning\"\n",
    "  >\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "38b44028-d0ca-4d22-a996-d65296863095",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 1 - Creating a Job Using the Lakeflow Jobs UI\n",
    "\n",
    "In this lesson, we will start by creating a job using a single notebook and SQL Query and exploring the Lakeflow Jobs UI.\n",
    "\n",
    "In this demonstration, we will walk through the process of creating and running a Lakeflow Job in Databricks. \n",
    "\n",
    "The demo will include:\n",
    "\n",
    "- Creating a new job with two tasks: one using a notebook and the other using a SQL query.\n",
    "- Modifying task configurations.\n",
    "- Exploring the Lakeflow Jobs UI to understand how to modify, monitor, and manage job runs.\n",
    "\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this lesson, you should be able to:\n",
    "- Schedule a notebook task and Sql task in a Databricks Workflow Job\n",
    "- Running a Job which have multiple task\n",
    "\n",
    "## Data Overview \n",
    "We are going to use a retail dataset for this course across all demos. We have three different dimensions/data available: **customers data, sales data, and orders data** for our retail dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "522da3fa-4bb3-4d30-b114-2b23202d7bae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## REQUIRED - SELECT CLASSIC COMPUTE (The cluster named 'labuser')\n",
    "\n",
    "Before executing cells in this notebook, please select your classic compute cluster in the lab. Be aware that **Serverless** is enabled by default.\n",
    "\n",
    "Follow these steps to select the classic compute cluster:\n",
    "\n",
    "\n",
    "1. Navigate to the top-right of this notebook and click the drop-down menu to select your cluster. By default, the notebook will use **Serverless**.\n",
    "\n",
    "2. If your cluster is available, select it and continue to the next cell. If the cluster is not shown:\n",
    "\n",
    "   - Click **More** in the drop-down.\n",
    "\n",
    "   - In the **Attach to an existing compute resource** window, use the first drop-down to select your unique cluster.\n",
    "\n",
    "**NOTE:** If your cluster has terminated, you might need to restart it in order to select it. To do this:\n",
    "\n",
    "1. Right-click on **Compute** in the left navigation pane and select *Open in new tab*.\n",
    "\n",
    "2. Find the triangle icon to the right of your compute cluster name and click it.\n",
    "\n",
    "3. Wait a few minutes for the cluster to start.\n",
    "\n",
    "4. Once the cluster is running, complete the steps above to select your cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2e002ff2-8bb0-478f-9b3d-2318547e02aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## A. Classroom Setup\n",
    "\n",
    "Run the following cell to configure your working environment for this course. It will also set your default catalog to **dbacademy** and the schema to your specific schema name shown below using the `USE` statements.\n",
    "<br></br>\n",
    "\n",
    "\n",
    "```\n",
    "USE CATALOG dbacademy;\n",
    "USE SCHEMA dbacademy.<your unique schema name>;\n",
    "```\n",
    "\n",
    "**NOTE:** The `DA` object is only used in Databricks Academy courses and is not available outside of these courses. It will dynamically reference the information needed to run the course.\n",
    "\n",
    "**NOTE:** If you want to use **Serverless** compute, make sure you are on the **latest version (version > 1)**. Otherwise, the setup will not work correctly.\n",
    "\n",
    "- [Select an environment version](https://docs.databricks.com/aws/en/compute/serverless/dependencies#-select-an-environment-version)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fedd9d7a-30a2-48a3-b11a-8cce53545cf5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./Includes/Classroom-Setup-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e9d09d0b-fa85-4463-8a8d-8672667c08c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## B. Explore Your Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eb01d96a-62c4-4699-a0a1-2efc7bf2259c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### B1. Explore your Class Schema\n",
    "\n",
    "Complete the following to explore your **dbacademy.labuser** schema:\n",
    "\n",
    "1. In the left navigation bar, select the catalog icon:  ![Catalog Icon](./Includes/images/catalog_icon.png)\n",
    "\n",
    "2. Locate the catalog called **dbacademy** and expand the catalog.\n",
    "\n",
    "3. Expand your **labuser** schema. \n",
    "\n",
    "4. Notice that within your schema no tables exist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "35df230b-0e64-469e-8b8a-a88b9cd6d30b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### B2. Explore your Source Catalogs\n",
    "\n",
    "#### dbacademy_bank Catalog\n",
    "\n",
    "Complete the following to explore your **dbacademy_bank** and **dbacademy_retail** catalogs. We will be ingesting tables and files from these locations during the demos and labs:\n",
    "\n",
    "1. In the left navigation bar, select the catalog icon:  ![Catalog Icon](./Includes/images/catalog_icon.png)\n",
    "\n",
    "2. Locate the catalog called **dbacademy_bank** and expand the catalog.\n",
    "\n",
    "3. Expand your **v01** schema. \n",
    "\n",
    "4. Notice that within your schema a single volume named **banking** exists with a CSV file.\n",
    "\n",
    "#### dbacademy_retail Catalog\n",
    "\n",
    "1. In the left navigation bar, select the catalog icon:  ![Catalog Icon](./Includes/images/catalog_icon.png)\n",
    "\n",
    "2. Locate the catalog called **dbacademy_retail** and expand the catalog.\n",
    "\n",
    "3. Expand your **v01** schema. \n",
    "\n",
    "4. Notice that within your schema:\n",
    "  - Multiple tables exist\n",
    "  - In **Volumes** two volumes exist: **retail-pipeline** and **source_files**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "30a3c906-861b-4275-9f23-955a9ef92084",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## C. Viewing Your Files \n",
    "Complete the following steps to review the notebook and SQL file you will use in this job. All files are located in the **Task Files** folder within the directory for the corresponding lesson number.\n",
    "\n",
    "### C1. Viewing Notebook File\n",
    "1. Navigate to (or click the link for) the notebook: [Task Files/Lesson 1 Files/1.1 - Creating orders table]($./Task Files/Lesson 1 Files/1.1 - Creating orders table).  \n",
    "  - Review the notebook and note that it reads data from **dbacademy_retail.v01.sales_orders** and creates a simple table named **orders_bronze** in your designated **dbacademy.labuser** schema.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5598d247-94b1-4b9c-bc3b-b3f651ac163c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### C2. Viewing and Saving SQL Query File\n",
    "#### ---------- PLEASE READ THE FOLLOWING THOROUGHLY TO AVOID ISSUES -----------\n",
    "\n",
    "1. Navigate to (or click the link for) the Lesson 1 Task file: [Task Files/Lesson 1 Files/]($./Task Files/Lesson 1 Files/). Then click on **1.2 - Creating Sales Table - SQL Query**.\n",
    "\n",
    "**TROUBLESHOOTING:**  \n",
    "If you see a file named **1.2 - Creating Sales Table - SQL Query.dbquery**, it means you are using **Serverless Version 1**. To fix this:\n",
    "\n",
    "- Delete the file **./Task Files/Lesson 1/1.2 - Creating Sales Table - SQL Query.dbquery** (the one with the `.dbquery` extension).\n",
    "\n",
    "- Then go back and run the **Classroom Setup** using the appropriate compute option:  \n",
    "    - Your **labuser** cluster, or  \n",
    "    - **Serverless** version greater than 1 — [Select an environment version](https://docs.databricks.com/aws/en/compute/serverless/dependencies#-select-an-environment-version)\n",
    "\n",
    "2. Review the SQL file, which reads data from **dbacademy_retail.v01.sales** and creates a table named **sales_bronze** in your **dbacademy.labuser** schema.\n",
    "\n",
    "3. Connect to your SQL Warehouse using the drop-down menu in the top-right section of the editor.\n",
    "\n",
    "4. **REQUIRED - Click on \"Save Icon\" (top right) to save this query**. You must save this query file to run it in a SQL query task.\n",
    "\n",
    "5. After reviewing the notebook and SQL file, close the files and return to this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0f43089b-b278-4c66-952b-94ca548b0d78",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## D. Create the Job\n",
    "\n",
    "Complete the steps below to create a Lakeflow Job with two tasks:\n",
    "\n",
    "- A notebook task  \n",
    "- A SQL file task\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9ffb0ee8-5f90-4e51-989a-529544f57e55",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### D1. Generate your Job Configuration\n",
    "\n",
    "1. Run the cell below to print out values you'll use to configure your job in subsequent steps. Make sure to specify the correct job name and Files.\n",
    "\n",
    "    **NOTE:** The `DA.print_job_config` object is specific to the Databricks Academy course. It will output the necessary information to help you create the job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1afaa670-1c82-45d9-a43c-11ac6e9e3f81",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "DA.print_job_config(job_name_extension='Demo_01_Retail_Job', \n",
    "                    file_paths='/Task Files/Lesson 1 Files',\n",
    "                    Files=[\n",
    "                        '1.1 - Creating orders table'\n",
    "                    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9b855402-1bb0-4c3a-95a3-a7177e0ed992",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### D2. Create and Name the Job\n",
    "\n",
    "Complete the following steps to create and name your job.\n",
    "\n",
    "1. Right-click the **Jobs and Pipelines** button in the sidebar and select *Open Link in New Tab*.\n",
    "\n",
    "2. In the new tab, confirm that you are in the **Jobs & Pipelines** tab.\n",
    "\n",
    "3. Click the **Create** button and select **Job** from the dropdown.\n",
    "\n",
    "4. In the top-left corner of the screen, you’ll see a default job name based on the current date and time (for example, *New Job Jul 29, 2025, 11:46 AM*).\n",
    "\n",
    "5. Change the **Job Name** to the one provided in the previous cell (for example: **Demo_01_Retail_Job_labuser123**).\n",
    "\n",
    "6. Leave the job open and proceed to the next steps.\n",
    "\n",
    "**NOTE:** If you click on a recommended task (like **Notebook**), you will be redirected to a different page than shown in the screenshot below.\n",
    "\n",
    "![Lesson01_Jobs_UI.png](./Includes/images/Lesson01_Jobs_UI.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "29ec94df-b552-4cd6-9736-00a0f4f6eef3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### D3. Create the Notebook Task\n",
    "\n",
    "Complete the following steps to add a notebook task.\n",
    "\n",
    "1. In the Lakeflow Jobs UI, You may see some task suggestion. For Eg., **Notebook** or **SQL File**\n",
    "\n",
    "2. Select the **Notebook** task type.\n",
    "\n",
    "3. Configure the task using the settings below:\n",
    "\n",
    "| Setting         | Instructions |\n",
    "|-----------------|--------------|\n",
    "| **Task name**   | Enter **ingesting_orders** |\n",
    "| **Type**        | Select **Notebook** |\n",
    "| **Source**      | Choose **Workspace** |\n",
    "| **Path**        | Use the file navigator to locate and select **Notebook #1**:<br>**./Task Files/Lesson 1 Files/1.1 - Creating orders table** |\n",
    "| **Compute**     | Select a **Serverless** cluster from the dropdown menu.<br>(We will use Serverless clusters for all jobs in this course. You may specify a different cluster outside of this course, if needed.) <br></br>**NOTE**: If you selected your all-purpose cluster, you may get a warning about how this will be billed as all-purpose compute. Production jobs should always be scheduled against new job clusters appropriately sized for the workload, as this is billed at a much lower rate.\n",
    " |\n",
    "| **Create task** | Click **Create task** |\n",
    "\n",
    "4. Keep the Lakeflow Jobs UI open, you’ll be adding another task in the next step.\n",
    "##### For better performance, please enable Performance Optimized Mode in Job Details. Otherwise, it might take 6 to 8 minutes to initiate execution.\n",
    "\n",
    "<br></br>\n",
    "\n",
    "#### Notebook Task Setup\n",
    "\n",
    "![Lesson01_Notebook_task.png](./Includes/images/Lesson01_Notebook_task.png)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e545f7b2-5f82-4566-bad2-429746e74f4c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### D4. Create the SQL Query Task\n",
    "\n",
    "Follow these steps to add a SQL file as a task:\n",
    "\n",
    "1. In the Lakeflow Jobs UI, click **Add task**.\n",
    "\n",
    "2. Select the **SQL query** task type.\n",
    "\n",
    "3. Configure the task using the settings below:\n",
    "\n",
    "| Setting           | Instructions |\n",
    "|-------------------|--------------|\n",
    "| **Task name**     | Enter **ingesting_sales** |\n",
    "| **Type**          | Select **SQL** |\n",
    "| **SQL task**      | Select **Query** |\n",
    "| **SQL query**     | From the dropdown, choose the SQL file:<br>**1.2 - Creating sales table - SQL Query** |\n",
    "| **SQL warehouse** | From the dropdown, select your SQL warehouse from drop-down menu |\n",
    "| **Depends on**    | No task should be selected here.<br>(Unselect **ingesting_orders** if it is selected.) |\n",
    "| **Create task**   | Click **Create task** |\n",
    "\n",
    "<br></br>\n",
    "\n",
    "#### SQL Task Setup\n",
    "\n",
    "![Lesson01_task1_sql.png](./Includes/images/Lesson01_task1_sql.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "489d156c-31e9-46f7-bf8b-9e38e030d8c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### D5. Explore and Modify the Job Details\n",
    "\n",
    "1. Navigate to the Job Details page. In the right pane, you will find the following job-level details:\n",
    "\n",
    "- **Job Details:** Information such as Job ID, creator, and more.\n",
    "- **Schedulers and Triggers:** View and configure various scheduling options and triggers for the job.\n",
    "- **Job Parameters:** Options to declare parameters that apply to the entire job.\n",
    "\n",
    "\n",
    "#### For better performance, please turn on Performance Optimized Mode in Job Details.\n",
    "\n",
    "##### Performance Optimized Mode\n",
    "- Enables fast compute startup and improved execution speed.\n",
    "\n",
    "##### Standard Mode\n",
    "- Disabling performance optimization results in startup times similar to Classic infrastructure and may reduce your costs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ef0d357d-1f63-40d1-a47f-d1bffe8e47c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## E. Run the Job\n",
    "\n",
    "1. In the upper-right corner, find the kebab menu (three dots) next to the **Run now** button. You will see options such as **Edit as YAML**, **Clone job**, **View as code**, and **Delete job**.\n",
    "\n",
    "2. Click **View as code** to see your job represented in three formats: YAML, Python (SDK and DABS), and JSON.\n",
    "\n",
    "3. Return to the main job page and click the **Run now** button in the top right to start the job.\n",
    "\n",
    "    **NOTE:** After starting the job, you can click the link to view the run in progress. In the next section, you will learn another way to view past and current job runs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "82faeb82-6bcb-4655-b1d8-e0fea47be6b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## F. Review the Job Run\n",
    "\n",
    "1. On the Job Details page, click the **Runs** tab in the top-left corner of the screen (you should currently be on the **Tasks** tab).\n",
    "\n",
    "2. In the Runs tab of your job, you can see detailed information about each run.\n",
    "   At the top, there is a time-based bar chart where:\n",
    "\n",
    "   - The X-axis represents each run.\n",
    "   - The Y-axis shows the time taken by each task within that run.\n",
    "3. Color Coding\n",
    "   -    key: green = success\n",
    "   -    red = failed\n",
    "   -    yellow = waiting/retry, \n",
    "   -    pink = skipped,\n",
    "   -    grey = pending/canceled/timeout.\n",
    "\n",
    "\n",
    "Below the chart, you will find a tabular matrix view that provides the same information in detail. This table starts with the timestamp and includes fields such as run_id, run status, duration, and other relevant details for each run.\n",
    "\n",
    "![Lesson01_view_runs.png](./Includes/images/Lesson01_view_runs.png)\n",
    "\n",
    "4. Open the output details by clicking the timestamp under the **Start time** column:\n",
    "\n",
    "   - If **the job is still running**, you will see the active state with a **Status** of **Pending** or **Running** in the right-side panel.\n",
    "\n",
    "   - If **the job has completed**, you will see the full execution results with a **Status** of **Succeeded** or **Failed** in the right-side panel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cfe3c801-791d-46f0-8836-d099ca7fae02",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## G. View Your New Tables\n",
    "1. From left-hand pane, select **Catalog**. Then drill down from **dbacademy** catalog.\n",
    "\n",
    "2. Expand your unique schema name.\n",
    "\n",
    "3. Notice that within your schema a table named **sales_bronze** and **orders_bronze**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2cb10138-9163-445f-a988-3a30efc551fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##H. Query Your New Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "110fff80-c99b-442e-b9c3-10a8117064c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Querying sales_bronze table\n",
    "SELECT * \n",
    "FROM sales_bronze\n",
    "LIMIT 50;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c0f53216-3b6e-412e-a68e-055b29bd7610",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Querying orders_bronze table\n",
    "SELECT * \n",
    "FROM orders_bronze\n",
    "LIMIT 50;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "26814ec3-d2c2-442b-bede-8fc936eccfa1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Additional Resources\n",
    "\n",
    "- [Lakeflow Jobs Documentation](https://docs.databricks.com/aws/en/jobs/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dce21c19-0f2d-43a0-b9ae-fbaaf7bc88dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "&copy; 2026 Databricks, Inc. All rights reserved. Apache, Apache Spark, Spark, the Spark Logo, Apache Iceberg, Iceberg, and the Apache Iceberg logo are trademarks of the <a href=\"https://www.apache.org/\" target=\"_blank\">Apache Software Foundation</a>.<br/><br/><a href=\"https://databricks.com/privacy-policy\" target=\"_blank\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\" target=\"_blank\">Terms of Use</a> | <a href=\"https://help.databricks.com/\" target=\"_blank\">Support</a>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {},
   "notebookName": "1 Demo - Creating a Job Using the Lakeflow Jobs UI",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
