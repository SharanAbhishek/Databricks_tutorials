{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6909beda-ca93-4b14-a250-319bffe39216",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img\n",
    "    src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\"\n",
    "    alt=\"Databricks Learning\"\n",
    "  >\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6df63b6c-9ae0-47da-b194-0e732d4e2c3f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 7 Bonus Lab - AUTO CDC INTO with SCD Type 1\n",
    "\n",
    "##### NOTE: The AUTO CDC APIs replace the APPLY CHANGES APIs, and have the same syntax. The APPLY CHANGES APIs are still available, but Databricks recommends using the AUTO CDC APIs in their place.\n",
    "\n",
    "### Estimated Duration: ~15-20 minutes\n",
    "\n",
    "#### This is an optional lab that can be completed after class if you're interested in practicing CDC.\n",
    "\n",
    "\n",
    "In this demonstration you will use Change Data Capture (CDC) to detect changes and apply them using SCD Type 1 logic (overwrite, no historical records).\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "By the end of this lesson, you will be able to:\n",
    "- Use `AUTO CDC INTO` to perform Change Data Capture (CDC) using SCD Type 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5729c1d4-38c2-4a05-87ed-acc5c2f1e5c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## REQUIRED - SELECT CLASSIC COMPUTE (your cluster starts with **labuser**)\n",
    "\n",
    "Before executing cells in this notebook, please select your classic compute cluster in the lab. Be aware that **Serverless** is enabled by default.\n",
    "\n",
    "Follow these steps to select the classic compute cluster:\n",
    "\n",
    "1. Navigate to the top-right of this notebook and click the drop-down menu to select your cluster. By default, the notebook will use **Serverless**.\n",
    "\n",
    "1. If your cluster is available, select it and continue to the next cell. If the cluster is not shown:\n",
    "\n",
    "    - In the drop-down, select **More**.\n",
    "\n",
    "    - In the **Attach to an existing compute resource** pop-up, select the first drop-down. You will see a unique cluster name in that drop-down. Please select that cluster.\n",
    "\n",
    "**NOTE:** If your cluster has terminated, you might need to restart it in order to select it. To do this:\n",
    "\n",
    "1. Right-click on **Compute** in the left navigation pane and select *Open in new tab*.\n",
    "\n",
    "1. Find the triangle icon to the right of your compute cluster name and click it.\n",
    "\n",
    "1. Wait a few minutes for the cluster to start.\n",
    "\n",
    "1. Once the cluster is running, complete the steps above to select your cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "400b2361-76f8-46f2-a1f5-25f1195d8c8d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## A. Classroom Setup\n",
    "\n",
    "Run the following cell to configure your working environment for this course.\n",
    "\n",
    "**NOTE:** The `DA` object is only used in Databricks Academy courses and is not available outside of these courses. It will dynamically create and reference the information needed to run the course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9d350618-0dc0-433d-ad0e-5dc05ee764d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./Includes/Classroom-Setup-7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "94f986b2-4e29-433c-9a87-c5a362c9097f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## B. SCENARIO\n",
    "\n",
    "Your data engineering team wants to build a Lakeflow Spark Declarative Pipeline to maintain a record of current employees without keeping historical data (SCD Type 1). The project has been started, but the final step is to update the silver table with the current employee records that have not yet been completed. \n",
    "\n",
    "There are already two files in a cloud storage location that contain information about employees and employee updates.\n",
    "\n",
    "### REQUIREMENTS:\n",
    "It’s your job to complete the Spark Declarative Pipeline by adding the `AUTO CDC` statement to perform SCD Type 1.\n",
    "\n",
    "Follow the steps below to complete your task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "36e9010c-f8fd-4490-bb3f-53469c178d37",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## C. Explore the Raw Data Source Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c9b736ac-0f88-4796-bc45-1b121666a83a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. Run the cell below to programmatically view the files in your `/Volumes/your-lab-catalog-name/default/lab_files` volume. Confirm that you see **employees_1.csv** and **employees_2.csv**.\n",
    "\n",
    "**NOTE:** You can also manually navigate to your **labuser.default.lab_files** volume and view the files in the volume.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "69272387-e6ad-4b2a-b548-a681ba6a85e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "spark.sql(f'LIST \"/Volumes/{DA.catalog_name}/default/lab_files\"').display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2003cba9-2df1-4564-95e7-91d8e4ed949d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. Query the 2 CSV files in that volume. \n",
    "\n",
    "  Notice the following:\n",
    "\n",
    "   - The files contain a list of employees.\n",
    "\n",
    "   - The **employees_1.csv** contains the initial employees.  \n",
    "\n",
    "   - The **employees_2.csv** contains an update, a delete, and two new employees.\n",
    "\n",
    "   - The **Operation** column provides information about the action for each record (new employee, update employee information, or delete employee).\n",
    "\n",
    "   - The **ProcessDate** column indicates when the records were processed (acts as a sequence column).\n",
    "\n",
    "   - In total, there are 10 rows.\n",
    "\n",
    "      - There are two duplicate **EmployeeID** values:\n",
    "        - **EmployeeID 1** – Sophia was an employee, then should be deleted.\n",
    "        - **EmployeeID 3** – Liam received a bonus, and his **Salary** needs to be updated.\n",
    "\n",
    "      - **Employee 6 & 7** - New employees from the **employees_2.csv** file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9038c881-ef1f-4983-b203-d57c5e68a60b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SELECT\n",
    "  _metadata.file_name as source_file, \n",
    "  *\n",
    "FROM read_files(\n",
    "  '/Volumes/' || DA.catalog_name || '/default/lab_files',\n",
    "  format => 'CSV'\n",
    ")\n",
    "ORDER BY source_file, EmployeeID, ProcessDate DESC;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cca7ba6d-4e5c-4e04-a641-f2faa1578382",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "3. Looking at the output from above, our final table after applying SCD Type 1 on the two files should:\n",
    "\n",
    "   - Contain 6 rows of data:\n",
    "      - remove the **EmployeeID** with a `null` value (removed with a data quality expectation)\n",
    "      - delete **EmployeeID** 1 (employee who left)\n",
    "\n",
    "   - **EmployeeID 3** should have a current salary of 100,000 and only one row of data.\n",
    "\n",
    "   - **EmployeeID 6 & 7** are new employees from **employees_2.csv** file.\n",
    "\n",
    "   - No historical data should be tracked.\n",
    "\n",
    "<br></br>\n",
    "\n",
    "**FINAL TABLE OUTPUT**\n",
    "| EmployeeID | FirstName | Country | Department | Salary | HireDate   | ProcessDate |\n",
    "|------------|-----------|---------|------------|--------|------------|-------------|\n",
    "| 2          | Nikos     | GR      | IT         | 55000  | 2025-04-10 | 2025-06-05  |\n",
    "| 3          | Liam      | US      | Sales      | **100000** | 2025-05-03 | **2025-06-22**  |\n",
    "| 4          | Elena     | GR      | IT         | 53000  | 2025-06-04 | 2025-06-05  |\n",
    "| 5          | James     | US      | IT         | 60000  | 2025-06-05 | 2025-06-05  |\n",
    "| 6          | Emily     | US      | Enablement | 80000  | 2025-06-09 | **2025-06-22**  |\n",
    "| 7          | Yannis    | GR      | HR         | 70000  | 2025-06-20 | **2025-06-22**  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1bc52948-c6e4-47f9-becc-54cdb6ba3eb7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## D. TO DO: Complete the Pipeline with SCD Type 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9ffb8ef0-d583-4225-96b8-4b0c1d53d66e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. Run the cell below to create your starter Spark Declarative Pipeline for this lab. The pipeline will set the following for you:\n",
    "    - Your default catalog: `labuser`\n",
    "    - Your configuration parameter: `source` = `/Volumes/dbacademy/ops/your-labuser-name`\n",
    "\n",
    "    **NOTE:** If the pipeline already exists, an error will be returned. In that case, you'll need to delete the existing pipeline and rerun this cell.\n",
    "\n",
    "    To delete the pipeline:\n",
    "\n",
    "    a. Select **Jobs and Pipelines** from the far-left navigation bar.  \n",
    "\n",
    "    b. Find the pipeline you want to delete.  \n",
    "\n",
    "    c. Click the three-dot menu ![ellipsis icon](./Includes/images/ellipsis_icon.png).  \n",
    "\n",
    "    d. Select **Delete**.\n",
    "\n",
    "**NOTE:**  The `create_declarative_pipeline` function is a custom function built for this course to create the sample pipeline using the Databricks REST API. This avoids manually creating the pipeline and referencing the pipeline assets.\n",
    "\n",
    "**NOTE:** The run the solution for this lab go to step **F. Lab Solution (OPTIONAL)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "80b5b90f-53dd-4745-bbd2-48b6ddfa2c0b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "create_declarative_pipeline(pipeline_name=f'7 - CDC Lab Starter Project - {DA.catalog_name}', \n",
    "                            root_path_folder_name='7 - CDC Lab Starter Project',\n",
    "                            catalog_name = DA.catalog_name,\n",
    "                            schema_name = 'default',\n",
    "                            source_folder_names=['cdc_type_1_pipeline'],\n",
    "                            configuration = {'source':f'/Volumes/{DA.catalog_name}/default/lab_files'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d8fe368f-56b7-491c-a99f-8533ee2bda75",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. Complete the following steps to open the starter Spark Declarative Pipeline project for this lab:\n",
    "\n",
    "   a. In the main navigation bar right-click on **Jobs & Pipelines** and select **Open in Link in New Tab**.\n",
    "\n",
    "   b. In **Jobs & Pipelines** select your **7 - CDC Lab Starter Project - labuser** pipeline.\n",
    "\n",
    "   c. **REQUIRED:** At the top near your pipeline name, turn on **New pipeline monitoring**.\n",
    "\n",
    "   d. In the **Pipeline details** pane on the far right, select **Open in Editor** (field to the right of **Source code**) to open the pipeline in the **Lakeflow Pipeline Editor**.\n",
    "\n",
    "   e. In the new tab you should see the folder: **cdc_type_1_pipeline**. \n",
    "\n",
    "   f. Open the **cdc_type_1_pipeline** folder and select the **cdc_employees.sql** notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cb34286d-90ca-4fad-95d0-cf95e6bc9823",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### TO DO: Review the code in the cdc_employees.sql file and complete the `AUTO CDC INTO` statement to perform SCD Type 1.\n",
    "- For simplicity in training, all code for the pipeline is in one file **cdc_employees.sql**.\n",
    "\n",
    "- Walk through the **cdc_employees.sql** file and read the comments.\n",
    "\n",
    "- The **bronze** and **silver** table code is completed for you. You just need to complete the `AUTO CDC INTO` statement.\n",
    "\n",
    "- If you need the solution to the pipeline you can view it in the **7 - CDC Lab Solution Project** folder.\n",
    "\n",
    "[AUTO CDC INTO (Lakeflow Spark Declarative Pipelines)](https://docs.databricks.com/gcp/en/dlt-ref/dlt-sql-ref-apply-changes-into)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a6e7c8aa-069d-4849-a1b3-198f69567faa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## E. Explore Your CDC SCD Type 1 Streaming Table\n",
    "\n",
    "After you have completed the `AUTO CDC INTO` statement in the **cdc_employees.sql** file, compare your results to the solution image below.\n",
    "\n",
    "**FINAL PIPELINE RUN**\n",
    "\n",
    "![Lab 7 Pipeline Run](./Includes/images/lab_7_pipelinerun.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "af673c06-ef66-4485-99a3-e4032eedc12f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. Run the cell below to view the data in your **lab_2_silver_db.current_employees_silver_demo7** streaming table that applied SCD Type 1, and compare it to the solution below.\n",
    "\n",
    "    Notice with SCD Type 1 no historical data is kept.\n",
    "\n",
    "**FINAL TABLE SOLUTION**\n",
    "| EmployeeID | FirstName | Country | Department | Salary | HireDate   | ProcessDate |\n",
    "|------------|-----------|---------|------------|--------|------------|-------------|\n",
    "| 2          | Nikos     | GR      | IT         | 55000  | 2025-04-10 | 2025-06-05  |\n",
    "| 3          | Liam      | US      | Sales      | **100000** | 2025-05-03 | **2025-06-22**  |\n",
    "| 4          | Elena     | GR      | IT         | 53000  | 2025-06-04 | 2025-06-05  |\n",
    "| 5          | James     | US      | IT         | 60000  | 2025-06-05 | 2025-06-05  |\n",
    "| 6          | Emily     | US      | Enablement | 80000  | 2025-06-09 | **2025-06-22**  |\n",
    "| 7          | Yannis    | GR      | HR         | 70000  | 2025-06-20 | **2025-06-22**  |\n",
    "\n",
    "**NOTE**: If you ran the solution pipeline, the streaming table is named **current_employees_silver_demo7_solution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d43967d9-5a84-42c9-b4a7-615a65165a5d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SELECT *\n",
    "FROM lab_2_silver_db.current_employees_silver_demo7\n",
    "ORDER BY EmployeeID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "da946e9a-0804-4e19-8f0c-5b91a335955c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## F. Lab Solution (OPTIONAL)\n",
    "If you want to run the solution, you can execute the cell below to create a pipeline using the **7 - CDC Lab Solution Project** project folder.\n",
    "\n",
    "Each table in this solution pipeline will end with **_solution** and the code cells below will need to be modified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "545709e3-f6be-4f07-8133-e22767034296",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "create_declarative_pipeline(pipeline_name=f'7 - CDC Lab Solution Project - {DA.catalog_name}', \n",
    "                            root_path_folder_name='7 - CDC Lab Solution Project',\n",
    "                            catalog_name = DA.catalog_name,\n",
    "                            schema_name = 'default',\n",
    "                            source_folder_names=['cdc_type_1_pipeline'],\n",
    "                            configuration = {'source':f'/Volumes/{DA.catalog_name}/default/lab_files'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6801e306-5ece-4815-b6b7-020edeb54358",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## G. CHALLENGE SCENARIO\n",
    "### Duration: ~10 minutes\n",
    "\n",
    "**NOTE:** *If you finish early in a live class, feel free to complete the challenge below. The challenge is optional and most likely won't be completed during the live class. Only continue if your Spark Declarative Pipeline pipeline was set up correctly in the previous section by comparing your pipeline to the solution image.*\n",
    "\n",
    "**SCENARIO:** In the challenge, you will land a new CSV file in your **lab_files** cloud storage volume and rerun the pipeline to watch the Spark Declarative Pipeline perform CDC SCD Type 1 on the new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "604b9d93-3920-4ece-8ef7-0fdb7d08fcbd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. Run the cell below to land another file in your **lab_files** cloud storage location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "51257b90-b51c-41ee-bab3-641b029c429c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "LabSetup.copy_file(copy_file = 'employees_3.csv', \n",
    "                   to_target_volume = f'/Volumes/{DA.catalog_name}/default/lab_files')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cc639527-4486-44f3-ad2d-6609abd4bb04",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. Query the **employees_3.csv** file. Notice the following:\n",
    "\n",
    "   - **EmployeeID** values **2** and **6** need to be removed.\n",
    "\n",
    "   - **EmployeeID 8** is a new employee in our company.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1840bb36-2cb7-4e66-b06a-3ddc7bf84b9d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SELECT \n",
    "  _metadata.file_name as source_file,\n",
    "  *\n",
    "FROM read_files(\n",
    "  '/Volumes/' || DA.catalog_name || '/default/lab_files/employees_3.csv',\n",
    "  format => 'CSV'\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dd9e5e48-ea2a-4b43-aebb-9f87b7d00413",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "3. Go back to your pipeline and select **Run pipeline**. Examine the pipeline run. Confirm it shows the following:\n",
    "\n",
    "![Lab 7 Challenge Run](./Includes/images/lab_7_challengesolution.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "926e4059-5577-4f3d-8121-833fdda5cc67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "4. Run the cell below to query the table **lab_2_silver_db.current_employees_silver_demo7** and view the results. Notice that:\n",
    "\n",
    "   - The two employees (**EmployeeID** 2 and 6) were deleted.\n",
    "\n",
    "   - **EmployeeID 8** was added.\n",
    "\n",
    "   - No historical data is kept with SCD Type 1.\n",
    "\n",
    "    **NOTE:** If you ran the solution pipeline, the streaming table is named **lab_2_silver_db.current_employees_silver_demo7_solution**.\n",
    "\n",
    "\n",
    "    **FINAL TABLE**\n",
    "| EmployeeID | FirstName  | Country | Department | Salary  | HireDate   | ProcessDate |\n",
    "|------------|------------|---------|------------|---------|------------|-------------|\n",
    "| 3          | Liam       | US      | Sales      | 100000  | 2025-05-03 | 2025-06-22  |\n",
    "| 4          | Elena      | GR      | IT         | 53000   | 2025-06-04 | 2025-06-05  |\n",
    "| 5          | James      | US      | IT         | 60000   | 2025-06-05 | 2025-06-05  |\n",
    "| 7          | Yannis     | GR      | HR         | 70000   | 2025-06-20 | 2025-06-22  |\n",
    "| 8          | Panagiotis | GR      | Enablement | 90000   | 2025-07-01 | 2025-07-22  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "809177e1-3986-4d35-9011-576a1b37132f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SELECT *\n",
    "FROM lab_2_silver_db.current_employees_silver_demo7\n",
    "ORDER BY EmployeeID;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "53545629-ba10-4ea2-ab8b-61b77d8dae30",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "&copy; 2026 Databricks, Inc. All rights reserved. Apache, Apache Spark, Spark, the Spark Logo, Apache Iceberg, Iceberg, and the Apache Iceberg logo are trademarks of the <a href=\"https://www.apache.org/\" target=\"_blank\">Apache Software Foundation</a>.<br/><br/><a href=\"https://databricks.com/privacy-policy\" target=\"_blank\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\" target=\"_blank\">Terms of Use</a> | <a href=\"https://help.databricks.com/\" target=\"_blank\">Support</a>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "sql",
   "notebookMetadata": {},
   "notebookName": "7 BONUS Lab - AUTO CDC INTO with SCD Type 1",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "SQL",
   "language": "sql",
   "name": "sql"
  },
  "language_info": {
   "name": "sql"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
